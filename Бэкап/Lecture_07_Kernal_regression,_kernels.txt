Машинное обучение, лекция 7 (Jaakkola)

Темы лекции:

От линейной регрессии к ядрам

Ядра, примеры, свойства

$\textbf{Линейная регрессия и ядра}$

Рассмотрим немного упрощенную модель, в которой мы опустим параметр отступа $\theta_0$, получив модель $y=\theta^T\phi(x)+\epsilon$, где $\phi(x)$ - это некоторая трансформация вектора признаков (например, полиномиальная).

Наша цель состоит в том, чтобы привести и оценку параметров модели и предсказание ответов к форме, ключающей только скалярное произведение между векторами признаков.

Мы уже подчеркивали важность регуляризации в сочетании с переводом примеров в более высоко размерное векторное пространство.

Регуляризированный метод наименьших квадратов с параметром $\lambda$ задается выражением

$$J(\displaystyle \theta)=\sum_{t=1}^{n}(y_{t}-\theta^{T}\phi(\mathrm{x}_{t}))^{2}+\lambda\Vert\theta\Vert^{2}\text{ (1)}$$

Эта формула может быть получена из записи для оценки логарифма правдоподобия (смотрите предыдущие лекции).

Эффект регуляризации заключается в том, чтобы заставлять параметры стремится к нулю.

Таким образом, параметры, связанные с любыми линейными измерениями, которые не помогают оучению модели, устанавливаются точно равными нулю.

Как и  ранее, оптимальные значения для $\theta$ следуют из приравнивания градиента к нулю.

$$\displaystyle \frac{dJ(\theta)}{d\theta}\ =\ -2\sum_{t=1}^{n}\overset{\alpha_{t}}{\overbrace{{(y_{t}-\theta^{T}\phi(\mathrm{x}_{t}))}}}\phi(\mathrm{x}_{t})+2\lambda\theta=0\text{ (2)}$$

Внимание! Этот перевод, возможно, ещё не готов.
Его статус: идёт перевод

Переведено на Нотабеноиде
http://translate.kursomir.ru/book/105/893

Переводчики: chargered


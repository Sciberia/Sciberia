Машинное обучение, лекция 2 (Jaakkola)

$\textbf{Алгоритм персептрон, сходимость и обобщающая способность.}$

Вспомним, что мы занимаемся линейными классификаторами, проходящими через начало координат, то есть

$$\textit{f}(\textbf{x};\theta )= \text{sign}(\theta ^{T}\textbf{x})\text{  (1)}$$

где $\theta\in  R^{d}$ - вектор параметров, которые мы хотим оценить на основе обучающих примеров (изображений)

$\textbf{x}_{1}, ..., \textbf{x}_{n}$ и меток $\text{y}_{1}, ..., \text{y}_{n}$.

Мы будем использовать алгоритм персептрон для решения данной задачи.

Пусть $\textit{k}$  обозначает число

обновлений параметров, которые мы провизвели, и  $\theta^{(k)}=0$ - вектор параметров после $\textit{k}$ обновлений.

В начале $\textit{k}=0$ и  $\theta^{(k)}=0$.

Затем алгоритм перебирает все тренировочные примеры $(\textbf{x}_t, y_{t})$ и обновляет параметры только в ответ на ошибки, когда метка предсказана некорректно.

Если говорить более точно, мы устанавливаем $\theta^{(k+1)} =\theta^{(k)} + y_{t}\textbf{x}_{t}$ тогда, когда $y_{t}(\theta^{(k)})^{T}\textbf{x}_{t}< 0$ (произошла ошибка), в противном случае оставляем пораметры без изменений.

$\textbf{Сходимость за конечное число обновлений}$

Покажем теперь, что алгоритм персептрон гарантированно сходится за конечное число шагов.

Подобный анализ также поможет нам понять, как линейный классификатор обобщает ранее не встречавшиеся изображения.

Для этого предположим, что все (обучающие) изображения имеют ограниченную норму в Евклидовом пространстве, т.е.  $\left \| \textbf{x}_{t} \right \|\leq R$ для всех t и некоторого конечного R.

Это очевидно в случае, когда каждый пиксель имеет ограниченное значение интенсивности.

Мы также сделаем более сильное предположение, что в нашем классе с конечными значениями параметров существует линейный классификатор, который верно классифицирует все (обучающие) изображения.

Говоря более формально, мы полагаем, что существует некоторая $\gamma > 0$ такая, что $y_{t}(\theta^{*})^{T}\textbf{x}_{t}\geq \gamma$ для любого t = 1, ...,n.

Дополнительный параметр $\gamma > 0$ используется, чтобы гарантировать, что

каждый пример классифицируется верно с некоторым $\textit{конечным отступом}$ (finite margin).

Прим.пер.Величина $y_{t}(\theta^{*})^{T}\textbf{x}_{t}$ действительно называется отступом объекта в русскоязычной литературе.

Доказательство сходимости опирается на два факта:

1)мы покажем, что скалярное произведение $(\theta^{*})^{T}\theta^{(k)}$ возрастает как минимум линейно с каждым обновлением и 2) что квадрат нормы $\left \| \theta ^{(k)} \right \|^{2}$ возрастает в лучшем случае линейно с числом обновлений k.

Объеденив эти два вывода, мы сможем показать, что косинус угла между $\theta^{(k)}$ и $\theta^{*}$ должен увеличиваться на некоторую конечную величину от обновления к обновлению.

Т.к. косинус ограничен еденицей, следовательно, мы можем сделать только конечное число обновлений.

Часть 1: просто возьмем скалярное произведение $(\theta^{*})^{T}\theta^{(k)}$ до и после каждого обновления.

Когда мы делаем обновление k , скажем из-за на изображении $\textbf{x}_{t}$, мы получим

$$(\theta^{*})^{T}\theta^{(k)}=(\theta^{*})^{T}\theta^{(k-1)}+y_{t}(\theta^{*})^{T}\textbf{x}_{t}\geq(\theta^{*})^{T}\theta^{k-1}+\gamma\textbf{(2)}$$

согдасно нашему предположению, что $y_{t}(\theta^{*})^{T}\textbf{x}_{t}\geq \gamma$ при любом t.

Таким образом, после k обновлений,

$$(\theta^{*})^{T}\theta^{(k)} \geq k \gamma \text{      (3)}$$

Наше следующее утверждение вытикает из того факта, что обновления происходят только в результате ошибок:

$$\begin{matrix} \left \| \theta ^{(k)} \right \|^{2} & =  & \left \| \theta ^{(k-1)}+y_{t}\textbf{x}_{t} \right \|^{2} & (4) \\  &=   & \left \| \theta ^{(k-1)} \right \|^{2}+2y_{t}(\theta^{k-1})^{T}\textbf{x}_{t}+\left \| \textbf{x}_{t} \right \|^{2} & (5)\\  & \leq  & \left \| \theta ^{(k-1)} \right \|^{2}+\left \| \textbf{x}_{t} \right \|^{2} &(6) \\  &\leq   & \left \| \theta ^{(k-1)} \right \|^{2}+R^{2} &(7) \end{matrix}$$

т.к. $y_{t}(\theta^{k-1})^{T}\textbf{x}^{t}< 0$ всегда, когда происходит обновление, и $\left \| \textbf{x}_{t} \right \|\leq R$ по предположению.

Таким образом, $\left \| \theta^{(k)} \right \|^{2} \leq kR^{2} \textbf{    (8)}$.

Теперь мы можем объеденить части 1) и 2), чтобы ограничить косинус угла между $\theta^{*}$ и $\theta^{(k)}$:

$$cos(\theta^{*}\theta^{(k)})=\frac{(\theta^{*})^{T}\theta^{(k)}}{\left \| \theta^{(k)} \right \|\left \| \theta^{*} \right \|}\geq^{1)}  \frac{k\gamma }{\left \| \theta^{(k)} \right \|\left \| \theta^{*} \right \|}\geq^{2)} \frac{k\gamma}{\sqrt{kR^{2}}\left \| \theta^{*} \right \|} \text{   (9)}$$

Т.к. косинус ограничен единицей, мы получаем

$$1\geq \frac{k\gamma }{\sqrt{kR^{2}}\left \| \theta ^{*} \right \|}\text{ or }k\leq \frac{R^{2}\left \| \theta^{*} \right \|^{2}}{\gamma ^{2}} \text{   (10)}$$

$\textbf{Геометрический смысл отступа}$

Полезно понять этот результат чуть глубже.

Например, показывает ли отношение $\left \| \theta ^{*} \right \|^{2}/\gamma ^{2}$ насколько задача классификации сложна.

Действительно показывает. Мы утверждаем, что величина $\gamma/\left \| \theta ^{*} \right \|$ - это минимальное расстояние от любого примера (изображения) до разделяющей плоскости, определяемой $\theta^{*}$.

Другими словами, это служит некоторой мерой того, как хорошо два класса разделяются (линейной границей).

Мы будем называть это геометрическим отступом $\gamma_{geom}$ (смотрите рисунок 1).

$\gamma_{geom}^{-1}$ - вполне удовлетворительная мера сложности задачи: меньший геометрический отступ, который разделяет обучающую выборку, означает более сложную задачу.

Чтобы вычислить $\gamma_{geom}$, нам необходимо измерить расстояние от разделяющей границы $(\theta^{*})^{T}\textbf{x}_{t}=0$ до одного из изображений $\textbf{x}_{t}$, для которого $y_{t}(\theta^{*})^{T}\textbf{x}_{t}=\gamma$

Рисунок 1. Геометрический отступ.

Т.к. $\theta^{*}$ задает нормаль к разделяющей границе, кратчайший путь от границы до изображения $\textbf{x}_{t}$ будет параллелен $\theta^{*}$.

Поэтому изображение, для которого $y_{t}(\theta^{*})^{T}\textbf{x}_{t}=\gamma$, наиболее близко к разделяющей границе.

Теперь определим отрезок из $\textbf{x}(0)= \textbf{x}_{t}$, параллельный $\theta^{*}$, направленный к разделяющей границе. 

Он задется уравнением $$\textbf{x}(\xi )=\textbf{x}(0)-\xi\frac{y_{t}\theta^{*} }{\left \| \theta^{*} \right \|}\text{   (11)}$$

где $\xi$ определяет длину отрезка, т.к. домножается на вектор  еденичной длины.

Остается найти значения $\xi$, такие, что $(\theta^{*})^T\textbf{x}(\xi)=0$ или, что эквивалетно,  $y_{t}(\theta^{*})^T\textbf{x}(\xi)=0$.

Это точка, где отрезок касается разделяющей границы. Таким образом

$$\begin{matrix}y_{t}\theta^{*T}\textbf{x}(\xi)&=&y_{t}\theta^{*T}\left[\textbf{x}(0)-\xi\frac{y_{t}\theta^{*}}{\left\|\theta^{*}\right\|}\right]&(12)\\ &=&y_{t}\theta^{*T}\left[\textbf{x}_{t}-\xi\frac{y_{t}\theta^{*}}{\left\|\theta^{*}\right\|}\right]&(13)\\ &=&y_{t}\theta^{*T}\textbf{x}_{t}-\xi\frac{\left\|\theta^{*}\right\|^{2}}{\left\|\theta^{*}\right\|}&(14)\\ &=&\gamma-\xi\left\|\theta^{*}\right\|=0&(15) \end{matrix}$$

это означает, что расстояние в точности равно $\xi=\gamma/\left \| \theta^{*} \right \|$, что и требовалось доказать.

Как следствие ограничение на число шагов алгоритма персептрон может быть записано более коротко в терминах геометрического отступа $\gamma_{geom}$ (расстояние до разделяющей границы):

$$k\leq\left(\frac{R}{\gamma_{geom}}\right)^{2}\text{(16)}$$

предполагая, что $\gamma_{geom}$ - наибольший геометрический отступ, который может быть достигнут линейным классификатором в данной задаче.

Стоит заметить, что результат не зависит (явным образом) ни от размерности d, ни от числа обучающих примеров n.

Тем не менее весьма заманчиво использовать $\left (\frac{R}{\gamma_{geom}} \right )^{2}$, как меру трудности (или сложности) задачи обучения линейного классификатора в данном контексте.

Позже в курсе вы увидите, что это действительно так, в терминах меры известной как размерность Вапника-Червоненкиса (VC-dimension).

$\textbf{Гарантия обобщающей способности}$

Мы довольно долго обсуждали персептрон только в отношении к обучающей выборке, но мы больше заинтересованны в том, как персептрон  классифицирует изображения, которые до сих пор не встречались, т.е. какова его обобщающая способность.

Наш простой анализ выше предоставляет некоторую информацию об обобщающей способности.

Давайте предположим, что все изображения и метки, которые мы могли бы встретить, удовлетворяют двум условиям.

1) $\left\|\textbf{x}_{t}\right\|\leq R$ и 2) $y_t(\theta^{*})^Tx_t\geq\gamma$ для всех t и некоторого конечного $\theta^{*}$.

И так, в сущности, мы предположили, что существует линейный классификатор, который хорошо работает для всех изображений и меток в данной задаче, мы просто не знаем, что это за линейный классификатор. 

Представим теперь, что берем изображения и метки по одному и производим только одно обновление на изображение в случае ошибочной классификации, и двигаемся дальше.

В таком случае мы будем многократно использовать один и тотже набор обучающих изображений.

Как много ошибок мы сделаем в этой бесконечной случайной последовательности изображений и меток, при наших двух условиях?

Уже известное число $k\leq \left (\frac{R}{\gamma_{geom}} \right )^{2}$.

Как только мы сделали такое количество ошибок мы будем классифицировать все новые изображения правильно.

Обеспечив выполнение двух условий, особенно второго, мы добились гарантии высокой обобщающей способности.

Здесь есть одно обязательное условие: алгоритм персептрон должен знать, что он совершил ошибку.

В конечном счете, ограничением является число обновлений, основанных на ошибках

$\textbf{Классификатор максимального отступа?}$

До этого мы использовали простой онлайн алгоритм - персептрон, чтобы оценить параметры линейного классификатора. 
Прим.пер.(Грубо говоря, алгоритмы, которые могут работать с обучающими примерами по одному называются онлайн алгоритмами (например, персептрон или нейронные сети), а те, которым нужна вся обучающая выборка целиком – оффлайн (например решающие деревья/decision tree).

Однако, изначально мы предполагали, что существует некоторый классификатор, который имеет большой геометрически отступ, т.е. чья разделяющая граница хорошо отделяет все тренировочные изображения (примеры). 

Можем ли мы найти такой классификатор с большим отступом непосредственно? Да, можем.

Этот классификатор известен, как Метод Опорных Векторов или SVM (Support Vector Machine) для краткости. Детали в следующей лекции.

Переведено на Нотабеноиде
http://translate.kursomir.ru/book/105/558

Переводчики: chargered, usopp


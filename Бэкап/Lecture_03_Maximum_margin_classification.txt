Машинное обучение, лекция 3 (Jaakkola)

$\textbf{Метод опорных векторов}$

До сих пор мы использовали предположение о существовании линейного классификатора с большим геометрическим отступом, т.е. чья разделяющая граница хорошо отделяет все тренировочные изображения (примеры).

Такой классификатор с большим отступом кажется тем, который мы бы предпочли к использованию.

Можем ли мы найти его непосредственно? Да, можем.

Этот классификатор известен, как Метод Опорных Векторов или SVM (Support Vector Machine) для краткости.

Вы моглибы предствить нахождение классификатора с максимальным отступом, сначала определив классификатор, который корректно классифицирует все примеры (рисунок 2a), и затем увеличивая геометрический отступ до тех пор, пока классификатор не застопорится в точке, где мы не можем больше увеличивать отступ (рисунок 2b). 

Это решение единственное в своем роде.

Рисунок 1.

Рисунок 1: a) Линейный классификатор с маленьким геометрическим отступом, b) линейный классификатор с максимальным отступом.

Более формально, мы можем поставить оптимизационную задачу непосредственно максимизации геометрического отступа.

Нам необходимо, чтобы классификатор давал правильные ответы на всей обучающей выборке или $y_{t}\theta^{T}\textbf{x}_{t}\geq \gamma$ для любого t = 1, . . . , n.

Согласно этому ограничению, мы хотим максимизировать $\gamma/\left \| \theta \right \|$, т.е. геометрический отступ.

Мы также можем минимизировать обратную величину $\left \| \theta \right \|/\gamma$ или квадрат этой величины $\frac{1}{2}(\left \| \theta \right \|/\gamma)^{2}$, подчиняющийся тем же условиям (множитель 1/2 включен лишь для дальнейшего удобства).

У нас получается следующая оптимизационная задача для нахождения $\hat{\theta}$:

минимизировать $\frac{1}{2}\left \| \theta \right \|^{2}/\gamma^{2}$ при условии $y_{t}\theta^{T}\textbf{x}_{t}\geq \gamma$ для всех $t=1,...,n\text{   (1)}$

В дальнейшем мы можем немного упростить эту задачу, избавившись от $\gamma$.

Давайте вначале перепишем оптимизационную задачу, чтобы выделить каким образом решение зависит (или не зависит) от $\gamma$.

минимизировать $\frac{1}{2}\left \| \theta \right \|^{2}/\gamma^{2}$ при условии $y_{t}(\theta/\gamma)^{T}\textbf{x}_{t}\geq 1$ для всех $t=1,...,n\text{   (2)}$

Иначе говоря, наша задача классификации (данные и постановка) говорит нам только об отношении $\theta/\gamma$, но не о $\theta$ или $\gamma$ по отдельности.

Например, геометрический отступ определяется только на основе этого отношения.

Умножение $\theta$ на константу также не изменяет разделяющую границу.

Поэтому можем положить $\gamma=1$ и решить оптимизационную задачу только для $\theta$:

минимизировать $\frac{1}{2}\left \| \theta \right \|^{2}$ при условии $y_{t}\theta^{T}\textbf{x}_{t}\geq 1$ для всех $t=1,...,n\text{   (3)}$

Эта оптимизационная задача и есть стандартная форма SVM и является задачей  $\textit{квадратичного программирования}$ (квадратичная целевая функция с линейными ограничениями).

Итоговый геометрический отступ равен $1/\left \| \hat{\theta} \right \|$, где $\hat{\theta}$ - единственное решение вышеуказанной задачи.

The decision boundary (separating hyper-plane) nor the value of the geometric margin were affected by our choice $\gamma = 1$.

$\textbf{Обобщенная формулировка, параметр смещения}$

Слегка изменим линейный классификатор, добавив параметр смещения (свободный член) так, что теперь разделяющая плоскость не обязана проходить через начало координат.

Другими словами, классификатор, который мы рассматриваем имеет форму

$f(\textbf{x};\theta,\theta_{0})=\text{sign}(\theta^{T}\textbf{x}+\theta_{0})\text{   (4)}$

с параметрами $\theta$ (нормаль к разделяющей гиперплоскости) и свободным членом $\theta_{0}$ - вещественное число.

Как и прежде, уравнение для разделяющей гиперплоскости получается приравниванием аргуманта сигнум функции к нулю или $\theta^{T}\textbf{x}+\theta_{0} = 0$.

Это общее уравнение для гиперплоскости (прямой в случае размерности 2).

Добавление свободного члена может привести к увеличению геометрического отступа классификатора.

Это продемонстрировано на рисунках 2a и 2b.

Заметим, что вектор $\hat{\theta}$ соответствует решению с максимальным отступом и отличается на двух рисунках.

Свободный член изменяет оптимизационную задачу лишь чуть-чуть:

минимизировать $\frac{1}{2}\left \| \theta \right \|^{2}$ при условии $y_{t}(\theta^{T}\textbf{x}_{t}+\theta_{0})\geq 1$ для всех $t=1,...,n\text{   (3)}$

Свободный член появляется только в ограничениях.

В этом и заключается отличие от простой модификации линейного классификатора, путем обучения его на примерах, которые имеют дополнительную константную компоненту, т.е. ${\textbf{x}}' = [\textbf{x}; 1]$.

В формулировке выше мы никак не предполагали, где разделяющая гиперплоскость должна появиться, только то, что она должна максимизировать геометрический отступ.

Рисунок 2.

Рисунок 2: a) линейный классификатор, проходящий через начало координат, б) линейный классификатор с параметром смещения.

$\textbf{Свойства линейного классификатора с максимальным отступом}$

Линейный классификатор максимального отступа имеет несколько очень приятных свойств, и некоторые не очень привлекательные особенности.

\textbf{Плюсы.} Во-первых мы уже определили этот классификатор, основанный на алгоритме персептрон, как классификатор с наилучшей разделяющей границой.

Также решении единственно для любого линейно разделимого обучающего множества.

Более того, построение разделяющей границы как можно дальше от обучающих примеров делает его устойчивым к шумным объектам (но все же не к шумной разметке).

Линейная разделяющая граница максимального отступа также имеет любопытное свойство, что решение зависит только от примеров, которые находятся точно на границах разделительной полосы (полоса, ограниченная пунктирными линиями, параллельными разделяющей границе, на рисунках)

Примеры, которые лежат точно на границе разделяющей полосы, называются $\textit{опорными векторами}$ (support vectors) (смотрите рисунок 3).

Остальные примеры могут лежать где угодно за предлелами разделяющей полосы без эффекта для решения.

Поэтому мы получили тот же самый классификатор, если бы имели только опорные вектора в качестве обучающей выборки.

Хорошо ли это? Чтобы ответить на этот вопрос, нам необходима более формальный (и честный) способ измерения насколько классификатор хорош.

Один из допустимых "честных" методов оценки метрики качества на основе только обучающей выборки называется $\textit{перекрестной проверкой}$ (cross-validation).

Это простой метод многократного обучения классификатора на различных подвыборках из обучающего множества и тестирования  на оставшихся отложенных (hold-out) примерах, которые классификатор не встречал при обучении.

Частный случай такого рода техники нахывается  $\textit{leave-one-out cross-validation}$.

Как и предполагает название, этот метод предполагает следующее: на роль отложенного тестового примера выбираем поочередно каждый из обучающих примеров, тренируем классификатор на основе оставшихся объектов, тестируем получившийся классификатор на отложенном примере и подсчитываем ошибки.

Рисунок 3.

Рисунок 3. Опорные вектора (кружки) для линейного классификатора максимального отступа.

Пусть верхний индекс '-i' обозначает параметры, которые мы получим путем нахождения линейного классификатора с максимальным отступом без $i^{th}$ тренировочного примера. Тогда

$\text{leave-one-out CV ошибка}=\frac{1}{n}\sum_{i=1}^{n}\text{Loss}\left ( y_{i},f(\textbf{x}_{i};\theta^{-i},\theta_{0}^{-i}) \right )\text{   (6)}$

где $\text{Loss}(y, {y}’)$ - бинарная функция потерь.

По сути, мы пытаемся оценить как хорошо классификатор обобщает каждый тренировочный пример, если бы он не был частью обучающей выборки.

Классификатор, который имеет низкую ошибку на leave-one-out cross-validation вероятно имеет хорошую обобщающую способность, хотя не гарантируется, что это так.

Какова ошибка на leave-one-out cross-validation у линейного классификатора с максимальным отступом?

Примеры, которые лежат вне разделяющей полосы будут классифицированы верно в независимости от того являются ли они частью обучающей всыборки или нет.

Для опорных векторов все иначе. Они определяют линейный классификатор и потому, будучи удаленными из обучающего множества, могут быть ошибочно классифицированны.

Поэтому мы можем определить простую оценку сверху для  leave-one-out CV ошибки:

$\text{leave-one-out CV ошибка}\leq \frac{\# \text{ of support vectors}}{n}\text{     (7)}$

Малое число опорных векторов означает разреженное и потому удачное решение.

Это еще один аргумент в пользу классификатора максимального отступа.

$\textbf{Недостатки}$. Отднако существуют и проблемы. Даже одиночный тренировочный пример, будучи неверно размеченным, может радикально изменить линейный классификатор максимального отступа.

Рассмотрим, например, что произойдет, если мы изменим метку верхнего правого опорного вектора на рисунке 3.

$\textbf{Релаксация, учет шумной разметки}$

Ошибки разметки - обычное явление во многих реальных задачах, и нам следует смягчить их эффект.

Обычно мы не знаем осложнена ли классификация примеров из-за ошибок разметки или они просто линейно не разделимы (не существует линейного классификатора, который может классифицировать их правильно).

В обоих случаях, мы должны  прийти к компромиссу между неверно распознанными обучающими примерами и потенциальной эффективностью на других объектах.

Возможно простейший способ позволить ошибки в линейном классификаторе с максимальным отступом заключается в том, чтобы ввести ослабляющие переменные для ограничений отступа в оптимизационной задаче.

Другими словами, мы измеряем степень с которой нарушается каждое ограничение отступа и назначаем штраф для этого нарушения.

Штраф за нарушение ограничений минимизируется вместе с нормой вектора параметров.

Из этого следует простая ослабленная оптимизационная задача:

$$\text{минимизировать}\frac{1}{2}\left \| \theta \right \|^{2}+C\sum_{t=1}^{n}\xi_{t}\text{  (8)}$$

$\text{ при условиях }y_{t}(\theta^{T}\textbf{x}_{t}+\theta_{0})\geq 1-\xi_{t}$
$\text{ и }\xi_{t}\geq 0\text{ для всех }t=1,...,n \text{  (9)}$

где $\xi_{t}$ - ослабляющие переменные.

Ограничения отступа нарушаются только в случае, если $\xi_{t}>0$ для некоторого примера.

Штраф за это нарушение $C\xi_{t}$ своего рода противовес к возможному выигрышу в минимизации квадрата нормы вектора параметров $\left \| \theta \right \|^{2}$.

Если мы увеличиваем штраф C за нарушение отступа, тогда в некоторый момент все  $\xi_{t}=0$  (если это возможно), и мы возвращаемся к классификатору с максимальным отступом.

С другой стороны, для малых C многие ограничения отступа могут быть нарушены.

Заметим, что ослабленная оптимизационная задача устанавливает специфичный количественно оцениваемый баланс между нормой вектора параметров и нарушением отступов.

Уместно спросить, тот ли это баланс, который мы хотим?

Давайте попробуем понять эту постановку чуть глубже.

Например, какой отступ будет в результате нарушения некоторых из ограничений?

Мы можем все еще использовать $1/\left \| \hat{\theta} \right \|$ в качестве геометрического отступа.

На самом деле это геометрический отступ, основанный на тех примерах, для которых $\xi_{t}^{*} = 0$, где * обозначает оптимальное значение параметров. В таком случае верно ли, что мы получили линейный классификатор с максимальным отступом для подмножества объектов, для которых $\xi_{t}^{*} = 0$?

Нет, это не так. Примеры, которые нарушают ограничения отступа, включая те, что классифицируются неверно (большое нарушение), имеют влияние на решение.

Другими словами вектор параметров $\hat{\theta}$ определяется на основе тех примеров, которые лежат точно на границах разделительной полосы, нарушают ограничения отступа, но не достаточно, чтобы быть неправильно классифицированными, и тех, которые классифицируются ошибочно.

В этом смысле такие объекты являются опорными векторами.

Внимание! Этот перевод, возможно, ещё не готов.
Его статус: идёт перевод

Переведено на Нотабеноиде
http://translate.kursomir.ru/book/105/781

Переводчики: chargered


Машинное обучение, лекция 5 (Jaakkola)

Линейная регрессия, активное обучение

Мы подошли к модели логистической регрессии, когда попытались выразить неуверенность линейного классификатора касательно меток объектов.

Такой же подход позволяет нам использовать предсказания линейной модели также в других различных контекстах.

Простейший пример такого использования - линейная регрессия, цель которой состоит в предсказании вещественного ответа $y_{t} \in  R$ для каждого входного вектора.

Чрезмерная фокусировка на линейности предсказаний не является ограничением по своему существу, т.к. линейная модель может быть легко расширена (об этом в следующей лекции).

Итак, каким образом нам следует моделировать вещественные ответы?

Линейная функция от входного вектора уже обеспечивает усредненное предсказание (mean prediction) или $\theta^{T}\textbf{x}+\theta_{0}$.

Рассматривая это более формально, мы утверждаем, что математическое ожидание значения ответа при некотором $x$ есть $\theta^{T}\textbf{x}+\theta_{0}$.

Или коротко, мы говорим, что $E\left(y|\textbf{x}\right) =\theta^{T}\textbf{x}+\theta_{0}$

Реальные ответы распределены вокруг такого усредненного предсказания.

Простейшее симметричное распределение - это нормальное (Гауссово) распределение.

Другими словами, мы говорим, что ответы $y$ принадлежат нормальной функции распределения

$N=(y;\mu,\sigma^{2})=\frac{1}{\sqrt{2\pi\sigma^{2}}}\text{exp}\left ( -\frac{1}{2\pi\sigma^{2}}(y-\mu^{2}) \right )\text{  (1)}$

где $\mu=\theta^{T}\textbf{x}+\theta_{0}$.

Наша модель потому определяется как

$P(y|\textbf{x}, \theta, \theta_{0}) = N(y; \theta^{T}\textbf{x}+\theta_{0}, \sigma^{2} )  \text{   (2)}$

Когда входные примеры имеют размерность единица, мы предсказываем усредненный ответ, который является прямой в пространстве $(x, y)$, и предполагаем, что шум в $y$ принадлежит нормальному распределению со средним значением 0 и дисперсией $\sigma^2$.

Заметим, что дисперсия шума $\sigma^2$ в модели не зависит от $x$.

Более того мы моделируем только дисперсию относительно $y$, в то время как предполагаем, что знаем значение $x$ с идеальной точностью.

Принимая во внимание влияние потенциального шума в $x$ на ответы $y$ хотелось бы учесть в параметрах $\theta$ и $\theta_0$ дисперсию шума $\sigma^2$, возможно, зависящую от значения $x$.

Конкретная форма влияния шума на параметры $\theta$ и $\theta_0$ зависит от вида шума, добавленного к $x$.

Позже мы обсудим это чуть более детально.

Мы можем также записать модель линейной регрессии другим способом, чтобы явно выразить, как в точности добавочный шум проявляется в ответах модели:

$y = \theta^{T}\textbf{x}+\theta_{0}+ \epsilon \text{  (3) }$

где $\epsilon \sim N(0, \sigma^{2})$ (означает, что шум $\epsilon$ нормально распределен со средним в нуле и дисперсией $\sigma^2$).

Очевидно, что  для такой модели $E\left({y|\textbf{x}}
\right) =\theta^{T}\textbf{x}+\theta_{0}$, т.к. мат. ожидание $\epsilon$ равно 0.

Более того, добавление Гауссово шума к однозначному предсказанию $\theta^{T}\textbf{x}+\theta_{0}$ обуславливает нормальное распределение $y$ со средним $\theta^{T}\textbf{x}+\theta_{0}$ и дисперсией  $\sigma^2$, как и раньше.

В частности, для обучаюих примеров $x_1, ..., x_n$ и ответов $y_1,...,y_n$ модель, связывающая их есть

$y_{t} = \theta^{T}\textbf{x}_{t}+\theta_{0}+ \epsilon_{t}, \text{ }  t = 1, ..., n \text{  (4)}$

где $\epsilon_t \sim N(0, \sigma^{2})$, и $\epsilon_i$ не зависит от $\epsilon_j$ для любых $i \neq j$.

Независимо от выбора способа записи модели (обе формы полезны), мы можем найти оценки параметров при помощи метода максимального правдоподобия.

По аналогии с логистической регрессией функционал правдоподобия записывается как

$L(\theta,\theta _{0},\sigma^{2})=\prod_{t=1}^{n}\frac{1}{2\pi\sigma ^{2}}\text{exp}\left ( -\frac{1}{2\sigma ^2}(y_{t}-\theta ^{T}\textbf{x}_{t}-\theta _{0})^{2} \right )\text{   (5)}$

Заметим, что $\sigma^2$ также является параметром модели, который мы должны оценить.

Этот параметр отвечает за детали, не учтенные в линейной модели.

В терминах логарифма правдоподобия, мы пытаемся максимизировать

$\begin{align}-l(\theta,\theta_{0},\sigma^{2}) &= \sum_{t=1}^{n}\text{log}\left [ \frac{1}{2\pi\sigma ^{2}}\text{exp}\left ( -\frac{1}{2\sigma ^2}(y_{t}-\theta ^{T}\textbf{x}_{t}-\theta _{0})^{2} \right ) \right ] & \text{  (6)}\\ &= \sum_{t=1}^{n}\left [\frac{1}{2}\text{log}(2\pi)-\frac{1}{2}\text{log}\sigma^{2}-\frac{1}{2\sigma ^2}(y_{t}-\theta ^{T}\textbf{x}_{t}-\theta _{0})^{2} \right ] &\text{  (7)}\\ &= \text{const-}\frac{n}{2}\text{log}\sigma^{2}-\frac{1}{2\sigma ^2}\sum_{t=1}^{n}(y_{t}-\theta ^{T}\textbf{x}_{t}-\theta _{0})^{2} &\text{  (8)}\end{align}$

где $const$ собирательный термин, который не зависит от параметров модели.

Теперь задача оценки параметров $\theta$ и $\theta_0$ отделена от оценки $\sigma^2$.

Т.е. мы можем найти оптимальные $\hat{\theta}$ и $\hat{\theta_0}$ простой минимизацией среднеквадратической ошибки

$\sum_{t=1}^{n}(y_{t}-\theta ^{T}\textbf{x}_{t}-\theta _{0})^{2} \text{  (9)}$

Возможно проще всего записать решение основанное на матричных вычислениях.

Положим $X$ - матрица, чьи строки $[\textbf{x}_{t}^{T} ,1]$ - это обучающие примеры с добавленной единицей на конце.

В терминах матричных вычислений оптимизационная задача принимает вид

$\begin{align} \sum_{t=1}^{n} \left ( y_{t}-\begin{bmatrix}\theta \\ \theta_{0}\end{bmatrix}^{T}\begin{bmatrix}\textbf{x}_{t}\\ 1\end{bmatrix} \right )^{2} & = &\sum_{t=1}^{n}\left ( y_{t}-[\textbf{x}_{t},1]\begin{bmatrix}\theta \\ \theta_{0}\end{bmatrix} \right )^{2}&\text{   (10)} \\&= & \left \| \begin{bmatrix}y_{1} \\ ...\\y_{n}\end{bmatrix}\begin{bmatrix}\textbf{x}_{1}^{T},1 \\ ... \\ \textbf{x}_{n}^{T},1\end{bmatrix}\begin{bmatrix}\theta \\ \theta_{0}\end{bmatrix} \right \|^{2} & \text{   (11)} \\&= & \left \| \textbf{y}-\textbf{X}\begin{bmatrix}\theta \\ \theta_{0}\end{bmatrix} \right \|^{2} & \text{   (12)} \\&= & \textbf{y}^{T}\textbf{y}-2\begin{bmatrix}\theta \\ \theta_{0}\end{bmatrix}^{T}\textbf{X}^{T}\textbf{y}+\begin{bmatrix}\theta \\ \theta_{0}\end{bmatrix}^{T}\textbf{X}^{T}\textbf{X}\begin{bmatrix}\theta \\ \theta_{0}\end{bmatrix} & \text{   (13)}\end{align}$

где $\textbf{y} = [y_{1}, ..., y_{n}]^{T}$ - вектор ответов на обучающих примерах. Оптимальные параметры даются формулой

$\begin{bmatrix}\hat{\theta} \\ \hat{\theta_{0}}\end{bmatrix}=(\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}\textbf{y} \text{   (14)}$

Отметим, что оптимальные значения параметров - это линейная функция от наблюдаемых ответов $y$.

Позже мы воспользуемся этим свойством.

Дисперсия шума может быть теперь установлена в соответсвии с остаточными ошибками предсказаний.

В действительности оптимальное значение для $\sigma^2$ дается выражением

$\hat{\sigma} ^{2}=\frac{1}{n}\sum_{t=1}^{n}(y_{t}-\hat{\theta}^{T}\textbf{x}_{t}-\hat{\theta _{0}})^{2} \text{   (15)}$

которое является среднеквадратической ошибкой предсказания.

Заметим, что мы не можем вычислить $\hat{\sigma}^2$ до того, как узнаем, насколько хорошо линейная модель описывает наблюдения.

Смещение и дисперсия оценки параметров.

Мы можем использовать формулу для оценки параметров, полученную в (14), чтобы проанализировать, как хороши эти оценки.

Для этой цели давайте довольно сделаем сильное предположение, что реальные соотношения между $x$ и $y$ линейны (мы просто не знаем истинных значений $\theta^*, \theta_{0}^{*}$, и $\sigma^{*2}$).

Поэтому мы можем описать наблюдаемые ответы $y_t$ как

$y_{t}=\theta^{*T}\textbf{x}_{t}-\theta _{0}^{*}+\epsilon_{t},\text{ } t=1,...,n \text{   (16)}$

где $\epsilon_{t} \sim N(0, \sigma^{*2})$.

Или в матричной форме

$\textbf{y}=\textbf{X}\begin{bmatrix}\theta^{*}\\\theta_{0}^{*}\end{bmatrix}+\textbf{e}\text{ }\text{  (17)}$

где $\textbf{e} = [\epsilon_{1}, ..., \epsilon _{n}]^{T}, E\left \{\textbf{e}\right \} = 0$ и $E\left \{  {\textbf{ee}^{T}} \right \}= \sigma^{*2} \textbf{I}$.

Вектор шума $e$ также не зависит от $X$.

Подставляя выражение для ответов в уравнение (14), получим

$\begin{align*}\begin{bmatrix}\hat{\theta}\\ \hat{\theta}_{0}\end{bmatrix}&=(\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}(\textbf{X}\begin{bmatrix}\theta^{*}\\ \theta_{0}^{*}\end{bmatrix}+\textbf{e})&\text{  (18)} \\  &=(\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}\textbf{X}\begin{bmatrix}\theta^{*}\\ \theta_{0}^{*}\end{bmatrix}+(\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}\textbf{e}&\text{  (19)} \\  &=\begin{bmatrix}\theta^{*}\\ \theta_{0}^{*}\end{bmatrix}+(\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}\textbf{e}&\text{  (20)}\end{align*}$

Иначе говоря, наши оценки параметров могут быть декомпозированы на сумму истинных параметров и оценок, основанных лишь на шуме (т.е. на векторе $e$).

Таким образом, условное математическое ожидание при фиксированном входе $X$

$E\left \{  \begin{bmatrix}\hat{\theta}\\ \hat{\theta}_{0}\end{bmatrix}|\textbf{X}\right \}=\begin{bmatrix}\theta^{*}\\ \theta_{0}^{*}\end{bmatrix}+(\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}E\left \{  \textbf{e}|\textbf{X}\right \}=\begin{bmatrix}\theta^{*}\\ \theta_{0}^{*}\end{bmatrix} \text{  (21)}$

Поэтому можно говорить, что наши оценки параметров несмещенные, т.е. совпадают с истинными при усреднении по всем возможным тренеровочным выборкам, которые могли бы быть получены.

Это предложение предлагаю опустить, т.к. по сути дублирует #71.

Используя выражения (20) и (21), мы также можем выразить условную дисперсию оценок параметров, где математическое ожидание снова зависит от шума в ответах.

$\begin{align*} Var\left \{  \begin{bmatrix}\hat{\theta}\\ \hat{\theta}_{0}\end{bmatrix}|\textbf{X}\right \}&= E\left \{ \left ( \begin{bmatrix}\hat{\theta}\\ \hat{\theta}_{0}\end{bmatrix}-\begin{bmatrix}\theta^{*}\\ \theta_{0}^{*}\end{bmatrix}|\textbf{X}\right )\left ( \begin{bmatrix}\hat{\theta}\\ \hat{\theta}_{0}\end{bmatrix}-\begin{bmatrix}\theta^{*}\\ \theta_{0}^{*}\end{bmatrix}\right )^{T}|\textbf{X}\right \}&\text{(22)}\\  &=E\left \{ ((\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}\textbf{e})((\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}\textbf{e})^{T} |\textbf{X}\right \}&\text{(23)}\\  &=E\left \{ (\textbf{X}^{T}\textbf{X})^{-1} \textbf{X}^{T}\textbf{e} \textbf{e}^{T}\textbf{X}(\textbf{X}^{T}\textbf{X})^{-1} |\textbf{X}\right \} &\text{(24)}\\  &= (\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}E\left \{ \textbf{e}\textbf{e}^{T} |\textbf{X}\right \}\textbf{X}(\textbf{X}^{T}\textbf{X})^{-1}&\text{(25)}\\  &= (\textbf{X}^{T}\textbf{X})^{-1} \textbf{X}^{T} (\sigma^{*2}\textbf{I})\textbf{X}(\textbf{X}^{T}\textbf{X})^{-1}&\text{(26)} \\  &= \sigma^{*2}(\textbf{X}^{T}\textbf{X})^{-1}\textbf{X}^{T}\textbf{X} (\textbf{X}^{T}\textbf{X})^{-1}&\text{(27)}\\  &= \sigma^{*2}(\textbf{X}^{T}\textbf{X})^ {-1} \end{align*}$

Итак, дисперсия параметров относительно шума в ответах - это функция от $X$.

Мы быдем использовать это свойство в следующей секции, чтобы выбрать входные данные так, чтобы улучшить качество оценки параметров или уменьшить дисперсию в предсказаниях.

Основываясь на значениях дисперсии и смещения, мы можем оценить среднеквадратическую ошибку в оценки параметров.

Для этого мы используем тот факт, что математическое ожидание квадрата нормы любого вектора, состоящего из случайных переменных, может быть декомпозировано на компоненты смещения и дисперсии в следующем виде: 

$\begin{align*}E\left \{ \left \| \textbf{z}-\textbf{z}^{*} \right \| \right \} 
   &= E\left \{ \left \| \textbf{z}-E\{\textbf{z}\} +E\{\textbf{z}\}-\textbf{z}^{*}\right \|^{2} \right \}&\text{ (29)}\\  
   &= E\left \{ \left \| \textbf{z}-E\{\textbf{z}\} \right \|^{2}+2(\textbf{z}-E\{\textbf{z}\})^{T}(E\{\textbf{z}\}-\textbf{z}^{*})+\left \| E\{\textbf{z}\}-\textbf{z}^{*} \right \|^{2} \right\}&\text{ (30)}\\  
   &= E\left \{ \left \| \textbf{z}-E\{\textbf{z}\} \right \|^{2}\right\}+2E\left \{ (\textbf{z}-E\{\textbf{z}\})^{T}\right\}(E\{\textbf{z}\}-\textbf{z}^{*})+\left \| E\{\textbf{z}\}-\textbf{z}^{*} \right \|^{2} \\  
   &= \overset{\text{variance}}{\overbrace{E\left \{ \left \| \textbf{z}-E\{\textbf{z}\} \right \|^{2}\right\}}}+\overset{\text{bias}^{2}}{\overbrace{\left \| E\{\textbf{z}\}-\textbf{z}^{*} \right \|^{2}}}&\text{ (31)} 
  \end{align*}$

где мы предполагаем $x^*$ фиксированым.

Убедитесь, что вы понимаете как это разложение было произведено.

Мы бы хотели более тщательно рассмотреть ту часть, которая связана с дисперсией, для лучшего применения результатов в нашем контексте:

$\begin{align*}E\left \{ \left \| \textbf{z}-E\{\textbf{z}\} \right \|^{2}\right\}&= E\left \{(\textbf{z}-E\{\textbf{z}\})^{T}(\textbf{z}-E\{\textbf{z} \})\right\} &\text{ (32)}\\  &= E\left \{Tr[(\textbf{z}-E\{\textbf{z}\})^{T}(\textbf{z}-E\{\textbf{z} \})]\right\} &\text{ (33)}\\ &= E\left \{Tr[(\textbf{z}-E\{\textbf{z}\})(\textbf{z}-E\{\textbf{z} \})^{T}]\right\} &\text{ (34)}\\  &= Tr\left [ E\left \{(\textbf{z}-E\{\textbf{z}\})(\textbf{z}-E\{\textbf{z} \})^{T}\right\} \right ] &\text{ (35)}\\ &= Tr[Var\{\textbf{z}\}]&\text{ (36)}\end{align*}$

где $Tr[\cdot]$ - след (trace) матрицы - сумма диагональных элементов, линейный оператор (поэтому мы смогли поменять его и математическое ожидание местами).  

Мы также использовали тот факт, что $Tr[\textbf{a}^{T}\textbf{b}] = Tr[\textbf{ab}^{T} ]$ для любых векторов $a$ и $b$.

Теперь, применяя данные выводы к (31), получим

$\begin{align*}E\left \{ \left \| \begin{bmatrix}\hat{\theta}\\ \hat{\theta_{0}}\end{bmatrix}-\begin{bmatrix}\theta^{*}\\ \theta_{0}^{*}\end{bmatrix} \right \| ^{2}|\textbf{X}\right \} &=\overset{\text{variance}}{\overbrace{Tr\left [ Var\left \{ \begin{bmatrix}\hat{\theta}\\ \hat{\theta_{0}}\end{bmatrix}|\textbf{X} \right \} \right ]}}+\overset{\text{bias}^{2}=0}{\overbrace{ \left \|  E\left \{ \begin{bmatrix}\hat{\theta}\\ \hat{\theta_{0}}\end{bmatrix}|\textbf{X} \right \}-\begin{bmatrix}\theta^{*}\\ \theta_{0}^{*}\end{bmatrix}\right \|^{2} }} \\  &= \sigma ^{*2}Tr[(\textbf{X}^{T}\textbf{X})^{-1}]&\text{(37)}\end{align*}$

Давайте чуть глубже поймем этит результат.

Как это зависит от числа обучающих примеров?

Другими словами, как быстро уменьшается среднеквадратическая ошибка при возрастании числа обучающих примеров, в предположении, что примеры $x$ выбираются независимо из некоторого распределения $P(x)$? 

Чтобы ответить на этот вопрос начнем с анализа матрицы $XX^T$:

$\begin{align*}\textbf{X}^{T}\textbf{X}  &=\sum_{t=1}^{n}\begin{bmatrix}\textbf{x}_{t}\\ 1\end{bmatrix}[\textbf{x}_{t}^{T},1] & \text{ (38)} \\  &= n\cdot \frac{1}{n}\sum_{t=1} ^{n}\begin{bmatrix}\textbf{x}_{t}\\ 1\end{bmatrix}[\textbf{x}_{t}^{T},1] & \text{ (39)}\\  &= n\cdot E_{x\sim P}\left \{\begin{bmatrix}\textbf{x}_{t}\\ 1 \end{bmatrix} [\textbf{x}_{t}^{T},1] \right \}=n\cdot \textbf{C}& \text{ (40)} \end{align*}$

Для больших $n$ среднеквадратическое отклонение оыенок параметров будет приблежаться к 

$\frac{\sigma ^{*2}}{n}\cdot Tr[\textbf{C}^{-1}]\text{ (41)}$

Таким образом, дисперсия оценок параметров будет вести себя как $\frac{\sigma^{*2}}{n}$.

Т.к. мы оценивали $d+1$ параметр, где $d$ - размерность входных данных, эту зависимость хотелось бы отразить в $Tr[C^{-1}]$. В действительности это так.

Данный термин - след матрицы $C^{-1}$ размером $(d+1)\times(d+1)$ прямо пропорционален $d+1$.

Когда число обучающих примеров мало, т.е. недостаточно велико по сравнению с числом параметров модели (размерности признакового пространства), часто полезно ввести регуляризацию оценки параметров.

Внимание! Этот перевод, возможно, ещё не готов.
Его статус: идёт перевод

Переведено на Нотабеноиде
http://translate.kursomir.ru/book/105/844

Переводчики: chargered, Fieldmarshal


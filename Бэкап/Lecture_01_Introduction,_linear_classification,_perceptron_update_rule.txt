Машинное Обучение, Лекция 1 (Jaakkola)

Пример

Начнем с примера.

Допустим, нам необходимо предоставить автоматический контроль доступа в здание.

Перед входом в здание каждый человек должен посмотреть в камеру для того, чтобы мы могли получить фотоснимок его лица.

Для наших целей достаточно решить по фотографии, может ли человек войти в здание. 

Может быть полезно (хотя бы попробовать) также идентифицировать каждого человека, но это может потребовать информацию, которой мы не обладаем (например, имена или соответствуют ли две фотографии одному и тому же человеку).

Мы имеем только фотографии лиц людей, полученные в то время, когда доступ все еще предоставлялся вручную.

В результате этой практики мы имеем $\textit{размеченные}$ (labeled) фотографии.

Фотография помечается как $\textit{положительная}$ (positive), если данный человек может пройти в здание и $\textit{отрицательная}$ (negaive) в противном случае.

Прим.пер. Такие названия меток изображений в случае, когда их всего две (пропустить или не пропустить человека в здание), связаны с тем, что для удобства математической записи, как это будет видно позже, одну метку обозначают +1, а другую -1.

В дополнении к набору отрицательно помеченных фотографий (т.к. мы ожидаем небольшое количество случаев запрета входа при нормальных условиях) мы можем использовать любые другие фотографии лиц людей, кого мы не собираемся пропускать в здание.

Желательно использовать фотографии, полученные с камер, похожим образом ориентированных относительно лица, (например с камер  из других зданий).

Наша задача состоит в том, чтобы построить функцию - $\textit{классификатор}$ (classifier), переводящую пиксели изображения в бинарные ($\pm$1) метки.

И у нас есть только небольшой набор размеченных фотографий (обучающая выборка), чтобы построить эту функцию.

Давайте сделаем эту задачу более формальной.

Положим, что каждая фотография (черно-белая) представлена вектор-столбцом x размерности d.

Таким образом, значения интенсивности пикселей в изображении, столбец за столбцом, сконцентрированы в одном вектор-столбце.

Если фотография имеет размер 100 х 100 пикселей, тогда d=10000.

Также мы предполагаем, что все фотографии одинакового размера.

Наш классификатор - бинарная функция f: $R^{d}\rightarrow \left \{ -1,1 \right \}$, выбранная только на основе обучающей выборки.

В нашей задаче мы предполагаем, что классификатор не знает ничего о фотографиях (или лицах, если на то пошло) вне размеченной обучающей выборки (labeled training set).

Так, к примеру, с точки зрения классификатора фотографии могли бы иметь характеристики такие как: ширина, высота и т.д. вместо интенсивности пикселей.

Классификатор имеет только набор n векторов $\textbf{x}_{1}...\textbf{x}_{n}$ бинарными $\pm$1 метками $y_{1}...y_{n}$.

Это единственная информация о задаче, которую мы можем использовать, чтобы определить функцию f.

$\textbf{Какое решение нам подходит?}$

Предположим теперь, что мы имеем n = 50 размеченных изображений размером 128 на 128 пикселей с интенсивностью в диапазоне от 0 до 255.

Вполне возможно, что мы сможем найти отдельный пиксель, скажем пиксель i, такой, что каждое из наших n изображений будет иметь уникальное значение этого пикселя.

Мы могли бы затем сконструировать простую бинарную функцию, основанную на этом отдельном пикселе, которая идеально переводит изображения из обучающей выборки в их метки.

Говоря другими словами, если $x_{ti}$ - это пиксель i 
$t^{го}$ обучающего изображения, и $x_{i}^{'}$ - $i^{ый}$ пиксель любого изображения $\textbf{x'}$, тогда

$$f_{i}\left ( x' \right )=\left\{\begin{matrix} y_{t}, \text{если }x_{ti}=x_{i}^{'} \text{ для некоторого t=1,...,n}\\ -1,\text{иначе}\end{matrix}\right.\text{ (1)}$$

казалось бы, что задача решена.

В действительности всегда можно построить такую "идеальную"  бинарную функцию, если изображения обучающей выборки различны (нет двух изображений, имеющих одинаковые интенсивности пикселей для всех пикселей).

Но можем ли мы ожидать, что такие правила будут полезными для изображений не из обучающей выборки?

Даже изображение одного и того же человека изменяется в чем-либо от кадра к кадру (ориентация немного отличается, условия освещения могли измениться и т.д.).

Данные правила не предоставляют разумных предсказаний для изображений, которые не идентичны  находящимся в обучающей выборке.

Основная причина, по которой такие тривиальные правила не подходят, это то, что наша задача заключается не в том, чтобы правильно классифицировать обучающую выборку.

Наша задача состоит в том, чтобы найти правило, которое хорошо работает для всех новых изображений, встречающихся в рамках управления доступом; обучающая выборка лишь полезный источник информации для поиска таких функций.

Выражаясь чуть более формально, мы хотели бы найти классификаторы, которые имеют хорошую $\textit{обобщающую способность}$ (generalize), например, классификаторы, чье качество на обучающей выборке показывает, как хорошо они работают для еще не встречавшихся изображений.

$\textbf{Выбор модели}$

Итак, каким образом мы можем найти классификаторы с хорошей обобщающей способностью?

Ключевая идея заключается в ограничении набора возможных бинарных функций, которые мы можем принимать во внимание.

Другими словами, мы хотели бы найти класс бинарных функций таких, что если функция, принадлежащая этому классу, работает  хорошо на обучающей выборке, она также вероятно будет хорошо работать и на новых изображениях.

"Правильный" класс функций для рассмотрения не может быть слишком большой в том смысле, что класс не может содержать слишком много явно отличающихся функций. 

Иначе мы, вероятно, найдем правила похожие на те тривиальные, которые близки к идеальным на обучающей выборке, но не обладают обобщающей способностью.

Класс функций не должен быть и слишком маленьким, или мы рискуем не найти ни одной функции в классе, которая бы работала хорошо даже для обучающей выборки.

Если они не работают хорошо на тренировочных данных, как мы можем ожидать хорошего качества на новых изображениях?

Нахождения класса функций - ключевая задача в машинном обучении, также известная как $\textit{задача выбора модели}$ (model selection problem).

$\textbf{Линейные классификаторы, проходящие через начало координат}$

Прим.пер. Здесь подразумеваются те линейные классификаторы, у которых свободный член равен 0, поэтому их разделяющие плоскости всегда проходят через начало координат.

Давайте теперь зафиксируем класс функций.

Например, мы будем рассматривать только класс $\textit{линейных классификаторов}$ (linear classifiers).

Такие пороговые линейные отображения изображений на метки.

Более формально мы рассматриваем только функции вида

$$f(\textbf{x},\theta )=\text{sign}(\theta _{1}x_{1}+...+\theta _{d}x_{d})=\text{sign}(\theta ^{T}\textbf{x}) \text{ (2)}$$

где $\theta =[\theta _{1},...,\theta _{d}]^{T}$ - вектор-столбец вещественно-значные параметров.

Различные значения параметров дают различные функции в заданном классе, то есть функции, которые могут принимать различные значения {$\pm$1, 1} на одном и том же входном изображении $\textbf{x}$ (т.е. значения функций могут отличаться для одного и того же входного изображения).

Иными словами, функции нашего класса параметризованы $\theta \in R^{d}$.

Мы также можем взглянуть на эти линейные классификаторы с геометрической точки зрения.

Классификатор изменяет его предсказания только когда аргумент сигнум функции (sign function) изменяется с позитивного на негативный (или наоборот).

Геометрически, в пространстве векторов изображений, этот переход соответствует пересечению $\textit{разделяющей границы}$, на которой аргумент сигнум функции в точности равен нулю, т.е. разделяющая граница - это множество всех $\textbf{x}$ таких, что $\theta ^{T}\textbf{x}=0$.

Упавнение определяет плоскость в d-мерном пространстве, плоскость, проходящая через начало координат, при $\textbf{x}$ = 0 удовлетворяет уравнению.

Вектор параметров $\theta$ - нормаль к данной плоскости (ортагонален ей); это очевитдно, т.к. плоскость определена, как все $\textbf{x}$, для которых $\theta ^{T}\textbf{x}=0$.

Вектор $\theta$ как нормаль к плоскости также определяет направление в пространстве изображений, вдоль которого занчение $\theta ^{T}\textbf{x}$ будет расти быстрее всего.

Рисунок 1 иллюстрирует эту идею.

Рисунок 1: Линейный классификатор, проходящий через начало координат

Перед тем как двигаться дальше, давайте разберемся, не потеряли ли мы полезную информацию об изображениях, ограничив себя линейными классификаторами? Вообще говоря, потеряли.

Рассмотрим например, как соседние пиксели на лице соотносятся друг с другом (например, кожа – пиксели последовательны, однородны и непрерывны).

Эта информация полностью потеряна.

Линейный классификатор абсолютно счастлив (то есть его способность классифицировать изображения остаются неизменными), если мы возьмем изображения, где позиции пикселей были переупорядочены, при условии, что мы применили такую же трансформацию ко всем изображениям. 

Эта перестановка пикселей лишь переупорядочит слагаемые в аргументе сигнум функции в уравнении (2).

Поэтому линейный классификатор не может использовать информацию о том, какие пиксели близки друг к другу в изображении.

$\textbf{Алгоритм обучения: персептрон}$

Теперь, когда мы выбрали класс функций (возможно субоптимальный), мы все еще должны найти конкретную функцию в классе, которая хорошо работает для обучающей выборки. 

Это часто называют $\textit{задачей оценивания}$.

Будем чуть более точными.

Мы хотим найти линейный классификатор, который допускает наименьшее число ошибок на обучающей выборке.

Другими словами, мы хотим найти $\theta$, который минимизирует $\textit{ошибку на обучении}$

$$\hat{E}(\theta )=\frac{1}{n} \sum_{n}^{t=1}\left ( 1 - \delta (y_{t}, f(\textbf{x};\theta )) \right )=\frac{1}{n} \sum_{n}^{t=1}\text{Loss}(y_{t},f(\textbf{x}_{t};\theta ))\text{      (3)}$$

Где $\delta(y,y')$=1, если y = y' и 0 в противном случае.

Ошибка на обучении просто показывает среднее число тренировочных изображений, на которых функция предсказывает метку отличную от истинной.

В общем случае, мы могли бы сравнить наши предсказания с истинными метками в терминах функции потерь. $\text{Loss}(y_{t},f(\textbf{x}_{t};\theta ))$

Это полезно когда ошибки определенного рода более важны, чем другие (к примеру, пропустить в здание человека, который не должен был пройти).

Для простоты мы используем бинарные потери, корые равны 1, если произошла ошибка и 0 в ином случае.

Как выглядит разумный алгоритм для нахождения параметров $\theta$?

К примеру, мы можем просто итеративно подстраивать параметры таким образом, чтобы скорректировать любые ошибки, которые совершает соответствующий классификатор.

Такой алгоритм кажется уменьшает ошибку на обучении, которая подсчитывает промахи.

Наверное, простейшим алгоритмом такого типа является $\textit{персептрон}$.

Мы рассматриваем каждое изображение из обучающей выборки по отдному, проходя через все изображения, и подстраиваем параметры классификатора согласно

${\theta}'\leftarrow\theta+y_{t}\textbf{x}_{t} \text{ если }y_{t} \neq f(\textbf{x}_{t}; \theta)$

Иначе говоря, параметры (классификатор) изменяются только если мы совершаем ошибку.

Эти обновления стремятся скорректировать ошибки.

Чтобы осознать это заметим, что когда мы допускаем ошибку знак $\theta^{T}\textbf{x}_{t}$ не совпадает с $y_{t}$, и произведение $y_{t}\theta^{T}\textbf{x}_{t}$ отрицательно; произведение положительно для верно классифицированных изображений.

Предположим, мы допустили ошибку на $\textbf{x}$.

Тогда новое значение параметров задается формулой ${\theta}'=\theta+y_{t} \textbf{x}_{t}$, записанной в векторной форме.

Если мы посмотрим на классификацию того же самого изображения ${x}_{t}$ после обновления, то увидим

$$y_{t}{\theta}'^{T}\textbf{x}^{t}=y_{t}(\theta+y_{t}\textbf{x}_{t})^{T}\textbf{x}_{t}=y_{t}\theta^{T}\textbf{x}_{t}+y_{t}^{2}\textbf{x}_{t}^{T}\textbf{x}_{t}=y_{t}\theta^{T}\textbf{x}_{t}+\left \| \textbf{x}_{t} \right \|^{2}\text{     (5)}$$

Другими словами значение $y_{t}\theta^{T}\textbf{x}_{t}$ возрастает в результате обновления параметров (становится более положительным).

Если мы рассмотрим одно и тоже изображение несколько раз, то мы будем непременно изменять параметры так, чтобы это изображение будет классифицироваться правильно, то есть значение $y_{t}\theta^{T}\textbf{x}_{t}$ станет положительным.

Ошибки на других изображениях могут направлять вектор параметров в различных направлениях, поэтому не очевидно, что алгоритм сходится к чем-нибудь практически применимому, если мы неоднократно пройдем по изображениям из обучающей выборки.

Анализ алгоритма Персептрон

Алгоритм персептрон останавливает обновление параметров, только если обучающая выборка классифицируется верно (нет ошибок - нет обновления).

Если обучающая выборка может быть классифицирована без ошибок с помощью линейного классификатора, найдет ли алгоритм персептрон такой классификатор?

Да, найдет, и он сойдется к такому классификатору за конечное число обновлений (ошибок).

Мы покажем это в лекции 2.

Переведено на Нотабеноиде
http://translate.kursomir.ru/book/105/548

Переводчики: chargered


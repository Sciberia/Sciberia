Машинное обучение, лекция 6 (Jaakkola)

$\textbf{Активное обучение (продолжение), нелинейные предсказания, ядра}$

Темы лекции:
1) Активное обучение
2) Нелинейные предсказания, ядра

$\textbf{Активное обучение}$

Мы можем использовать выражение для средне среднеквадратической ошибки, чтобы по возможности разумно выбирать входные данные $\textbf{x}_{1},…,\textbf{x}_{n}$ так, чтобы уменьшить конечную ошибку оценивания.

Это задача активного обучения.

Позволив методу управлять выбором обучающих примеров (входных данных), мы, в общем случае, обойдемся гораздо меньшим числом примеров в сравнении со случайной выборкой из некоторого распределения, базы данных, или случайно проведенных экспериментов.

Чтобы развить эту идею дальше, вспомним, что мы продолжаме полагать, что ответ $y$ происходит из линейной модели $y=\theta^{*T}\textbf{x}+\theta_{0}^{*}+\epsilon$, где $\epsilon\sim N(0, \sigma^{*2})$.

Мы ничего не предполагаем о распределении $x$, т.к. выбор входных данных в нашем распоряжении.

Для любого данного набора $\textbf{x}_{1},…,\textbf{x}_{n}$, в прошлый раз мы вывели выражение для среднеквадратической ошибки параметров $\hat{\theta}$ и $\hat{\theta_0}$, полученных методом максимального правдоподобия:

$E=\left \{ \left \| \begin{bmatrix} \hat{\theta}\\ \hat{\theta}_{0}\end{bmatrix}-\begin{bmatrix}\theta^{*}\\\theta_{0}^{*}\end{bmatrix}\right\|^{2}|\textbf{X}\right\}=\sigma^{*}Tr[(\textbf{X}^{T}\textbf{X})^{-1}]  (1)$

где математическое ожидание оценивается на основе различных обучающих выборок, полученных из линейной модели.

Мы не знаем дисперсию шума $\sigma^{2}$, но она входит в выражение только в виде мультипликативной константы и потому не влияет на выбор входных данных.

Когда выбор входных данных зависит от нас (например, какие эксперименты следует провести), мы можем выбрать те примеры, которые минимизируют $Tr[(\textbf{X}^{T}\textbf{X})^{-1}]$

Важная оговорка здесь: этот подход опирается на линейную зависимость между примерами и ответами.

В случае, когда это не так, мы можем остановиться на неоптимальном выборе входных данных.

Если дан критерий выбора, как нам следует искать, скажем, $n$ входных приверов $\textbf{x}_{1},…,\textbf{x}_{n}$, чтобы минимизировать этот критерий?

Простой способ заключается в том, чтобы выбирать примеры один за другим, слегка оптимизируя текущее значение критерия лишь выбором следующего примера. 

Давайте предположим, что мы уже имеем некоторый $X$ и, таким образом, знаем $\textbf{A}=(\textbf{X}^{T}\textbf{X})^{-1}$.

Теперь мы попытаемся выбрать еще один $x$ и добавить строку $[x^T, 1]$ к $X$.

В контексте конкретного приложения мы обычно ограничены в том, какие $x$ нам доступны (напрмер, из-за экспериментальной уcтановки).

Мы обсудим простые ограничения ниже. Давайте сейчас оценим эффект от добавления новой строки:

$\begin{bmatrix}\textbf{X}\\\textbf{x}^{T}1\end{bmatrix}^{T}\begin{bmatrix}\textbf{X}\\\textbf{x}^{T}1\end{bmatrix}=\textbf{X}^{T}\textbf{X}+\begin{bmatrix}\textbf{x}\\1\end{bmatrix}\begin{bmatrix}\textbf{x}\\1\end{bmatrix}^{T}=\textbf{A}^{-1}+\textbf{vv}^{T}(2)$

где $\textbf{v} = [x^{T} , 1]^{T}$. Мы хотим найти такое $v$, которое минимизирует

$Tr[(\textbf{A}^{-1}+\textbf{vv}^{T})^{-1}] (3)$

Обращение матрицы может быть на самом деле представлено в другой форме (достаточно просто для проверки)

$(\textbf{A}^{-1}+\textbf{vv}^{T})^{-1}=\textbf{A}-\frac{1}{(1+\textbf{v}^{T}\textbf{Av})}\textbf{Avv}^{T}\textbf{A} (4)$

чтобы след матрицы можно было записать как

$\begin{align*}Tr[(\textbf{A}^{-1}+\textbf{vv}^{T})^{-1}] &=Tr[\textbf{A}]-\frac{1}{1+\textbf{v}^{T}\textbf{Av}}Tr[\textbf{Avv}^{T}\textbf{A}] &(5)\\  &=Tr[\textbf{A}]-\frac{1}{1+\textbf{v}^{T}\textbf{Av}}Tr[\textbf{v}^{T}\textbf{AAv}] &(6)\\  &=Tr[\textbf{A}]-\frac{\textbf{v}^{T}\textbf{AAv}}{1+\textbf{v}^{T}\textbf{Av}} &(7) \end{align*}$

Заметим, что т.к. $Tr[\textbf{A}] = Tr[(\textbf{X}^{T}\textbf{X})^{-1}]$ означает среднеквадратическую ошибку до добавления нового примера, любой выбор $v$, например, добавление любого примера $x$ будет уменьшать ее.

Мы заинтересованы в нахождении единственного примера, который уменьшает среднеквадратическую ошибку наибольшим образом. Этот пример максимизирует

$\frac{\textbf{v}^{T}\textbf{AAv}}{1+\textbf{v}^{T}\textbf{Av}} (8)$

Насколько сильно мы можем уменьшить квадратическую ошибку? Выражение выше ограничено наибольшим собственным значением матрицы $A$.

Другими словами, с каждым новым примеров мы можем удалить в лучшем случае одну степень свободы из параметрического пространства.

Если мы не налагаем ограничений на выбор $v$, максимизирующий вектор будет бесконечной длины, сонаправленный собстенному вектору матрицы $A$, соответвующему наибольшему собственному числу (все собственные числа $A$ положительны, т.к. $A$ обратная к положительно определенной матрице $X^TX$).

Для линейной регрессии выгодно иметь входные примеры как можно дальше друг от друга (см. рисунок 1).

Если мы ограничим $\left \| \textbf{v} \right \|\leq c$, то максимизирующий $v$ будет собственный вектор $A$, соответствующий наибольшему собственному числу, нормализованный так, чтобы $\left \| \textbf{v} \right \|= c$.

(Рисунок 1)

Рисунок 1: a) параметры линейной модели очень чувствительны к шуму, когда входные примеры близки друг к другу; b) шум меньше влияет на параметры линейной модели, когда входные примеры далеки друг от друга.

Заметим, что может оказаться невозможно выбрать такой собственный вектор, т.к. $\textbf{v} = [\textbf{x}^{T} , 1]^{T}$.

Другие условия на $x$ будут также  ограничивать $v$.

Давайте рассмотрим простой пример, чтобы проиллюстрировать критерий отбора. Предположим мы имеем одномерную модель регрессии $y = \theta x + \theta_{0}+\epsilon$, где $x$ лежит на отрезке $\left[-1, 1\right]$.

Допустим, мы получили ответы для $x_1=1$ и $x_2=-1$. Таким образом

$\textbf{X}=\begin{bmatrix}1&1\\-1&1\end{bmatrix},\textbf{X}^{T}\textbf{X}=\begin{bmatrix}2&0\\0&2\end{bmatrix},\textbf{A}=(\textbf{X}^{T}\textbf{X})^{-1}\frac{1}{2}\begin{bmatrix}1&0\\0&1\end{bmatrix} (9)$

$\textbf{v} = [x, 1]^{T}$ и потому $\textbf{v}^{T} \textbf{Av} = (x^{2} + 1)/2$ и $\textbf{v}^{T} \textbf{AAv} = (x^{2} + 1)/4$. Максимизируемый критерий записывается, как

$\frac{\textbf{v}^{T} \textbf{AAv}}{1+\textbf{v}^{T}\textbf{Av}}=\frac{(x^{2} + 1)/4}{1+(x^{2} + 1)/2} (10)$

Т.к. $z/(1 + z)$ возрастающая функция $z$, данный критерий максимален, когда $(x^{2} + 1)/2 $ максимально.

При данных условиях максимум достигается в $x=1$ или $x=-1$.

Оба варианта одинаково хороши, но после выбора одного, другой станет предпочтителен на следующем шаге.

Результат согласуется с интуитивным предположением, что для линейной модели входные примеры должны быть как можно дальше друг от друга (см. рисунок 1).

До этого мы использовали среднеквадратическое отклонение параметров в качестве критерия выбора.

Как на счет разброса в предсказаниях модели? Давайте теперь поробуем найти $x$, ответ на котором наиболее неоднозначный. 

Снова запишем $\textbf{v} = [\textbf{x}^{T} , 1]^{T}$, так что

$\begin{align*}Var\{y|\textbf{x},\textbf{X}\}&=E\left\{\left(\hat{\theta}\textbf{x}+\hat{\theta}_{0}-\theta^{*T}\textbf{x}-\theta_{0}^{*}\right)^{2}|\textbf{x},\textbf{X}\right\}&(11)\\&=E\left \{ \begin{bmatrix}\textbf{x}\\1\end{bmatrix}^{T}\left(\begin{bmatrix}\hat{\theta}\\\hat{\theta}_{0}\end{bmatrix}-\begin{bmatrix}\theta^{*}\\ \theta_{0}^{*}\end{bmatrix}\right)\left(\begin{bmatrix}\hat{\theta}\\\hat{\theta}_{0}\end{bmatrix}-\begin{bmatrix}\theta^{*}\\\theta_{0}^{*}\end{bmatrix}\right)^{T}\begin{bmatrix}\textbf{x}\\1\end{bmatrix}|\textbf{x},\textbf{X} \right \}&(12)\\&=\begin{bmatrix}\textbf{x}\\1\end{bmatrix}^{T}\sigma ^{*2}(\textbf{X}^{T}\textbf{X})^{-1}\begin{bmatrix}\textbf{x}\\1\end{bmatrix}&(13)\\&=\sigma ^{*2}\cdot\textbf{v}^{T}\textbf{Av}&(14)\end{align*}$

где математическое ожидание оценивается на основе различных обучающих выборок, снова полученных из линейной модели.

Наибольшему разбросу соответствует $x$, максимизирующий $\textbf{v}^{T}\textbf{Av}$, где $\textbf{v} = [\textbf{x}^{T} , 1]^{T}$.

В случае без ограничений, когда на $v$ не налагаются условия, $x$, доставляющий данному критерию максимум, в точности совпадает с тем, который был бы выбран предыдущим критерием отбора.

$\textbf{Нелинейные предсказания, ядра}$

По большому счету все, что мы только что обсудили может быть обобщено на нелинейные модели, те модели, которые все еще линейно зависят от своих параметров, но производят нелинейные операции в исходном входном пространстве.

Это достигается переводом входных примеров в признаковое пространство с большим числом измерений, где новые признаки включают нелинейные преобразования исходных прзнаков.

Простейшим примером для демонстрации этого является одномерная модель линейной регрессии.

Рассмотрим линейную модель $y = \theta x + \theta_{0} + \epsilon$, где $\epsilon\sim N(0,\sigma^{2})$.

Мы можем получить квадратичную модель, переведя $x$ в расширенное признаковое пространство, добавив признаки квадратичные от $x$.

Модели третьей степени могут быть построены включением всех членов третьей степени и т.д.

Ясно, что мы можем делать линейные предсказания для векторов признаков:

$\begin{align*}  x&\overset{\phi }{\rightarrow} [1,\sqrt{2}x,x^{2}]^{T}=\phi (x) & (15)\\  x&\overset{\phi }{\rightarrow}[1,\sqrt{3}x,\sqrt{3}x^{2},x^{3}]^{T} & (16)\\  &... & (17) \end{align*}$

Роль множителя $\sqrt{2}$ скоро станет ясна.

Новая полиноминальная модель регрессии задается как

$y = \theta^{T}\phi(x) + \theta_{0} + \epsilon, \epsilon\sim N(0,\sigma^{2}) (18)$

(Рисунок 2)

Рисунок 2: a) линейная модель; b) Полиномиальная модель 3ей степени на тех же данных; c) Полиномиальная модель 5ой степени; d) Полиномиальная модель 7ой степени.

где размерность $\phi(x)$ (и потому $\theta$) зависит от порядка полинома.

Регуляризация параметров почти всегда используется совместно с высокоразмерными векторами признаков.

Без регуляризации модели высокого порядка могут серьезно переобучиться (overfit) к особенностям обучающей выборки.

Примеры обученной модели полиноминальной регрессии без регуляризации показаны на рисунке 2a-d (данные были сгенерированы из линейной модели).

Заметим, что все эти модели линейны в пространстве параметров, но нелинейны в исходном признаковом пространстве $x$, и оставляют неизменной стандартную модель линейной регрессии на рисунке 2a.

Полиномиальное расширение признакового пространства исходных векторов работает также для случаев большей размерности, т.е.

$\textbf{x}=[x1, x2]^{T}\overset{\phi }{\rightarrow}[1,x_{1}, x_{2}, \sqrt{2} x_{1} x_{2}, x_{1}^{2}, x_{1}^{2}]^{T}=\phi(\textbf{x}) (19)$

Один из недостатков такого подхода заключается в том, что размерность расширенного признакового простраства растет довольно быстро с ростом степени полинома, особенно, когда размерность исходного пространства и так велика.

Таблица ниже дает некоторое представление этого эффекта, который становится гораздо более выраженным в случае высокой размерности исходного признакового пространства.

Можем ли мы как-либо избежать увеличения размерности признакового пространства? Для моделей, которые мы только что обсуждали так долго, ответ положительный.

Мы можем определить расширенные вектора признаков косвенно через их скалярное произведение или функцию ядра (kernel).

Чтобы почувствовать силу такого подхода, давайте вычислим скалярное произведение между двумя векторами признаков, соответствующих полиномиальному расширению одномерного вектора, показанному ранее.

$\begin{align*}\phi(x) &= [1,\sqrt{3}x,\sqrt{3}x^{2},x^{3}]^{T}, &(20) \\ \phi({x}') &=[1,\sqrt{3}{x}',\sqrt{3}{x}'^{2},{x}'^{3}]^{T}&(21) \\ \phi(x)^{T}\phi({x}') &= 1+3x{x}'+3(x{x}')^{2}+(x{x}')^{3}=(1+x{x}')^{3}&(22)\end{align*}$

Видно, что мы можем сокращенно вычислить скалярное произведение между полиномиальными признаковыми векторами.

Мы должны были определить соответствующие константы в векторе признаков, чтобы получить данный результат.

Данный эффект все более явно выражен с ростом степени полинома и размерности исходного пространства признаков.

Для использования преимуществ данного подхода, мы, очевидно, должны сначала свести задачу оценки параметров к форме, содержащей только скалярные произведения (ядра) между признаковыми векторами.

$\begin{matrix} dim(\textbf{x})=2 &  &  &dim(\textbf{x})=3 \\ \text{degree }p & \#\text{ of features} & \text{degree }p & \#\text{ of features} \\  2&  6&  2& 10\\  3&  10&  3& 20\\  4&  15&  4& 35\\  5&  21&  5& 56 \end{matrix}$

$\textbf{Линейная регрессия и ядра}$

Давайте слегка упростим модель, отказавшись от свободного члена $\theta_0$, приведя модель к виду  $y = \theta^{T}\phi(\textbf{x})+\epsilon$, где $\phi(\textbf{x})$ - это некоторое преобразование признакового пространства (например полиномиальное).

Наша цель заключается в том, чтобы преобразовать задачи оценки параметров модели и активного обучения в форму, которая использует только скалярное произведение между векторами признаков. 

Мы уже подчеркивали, что необходимо использовать регуляризацию в сочетании с увеличением размерности признакового пространства. 

Регуляризованный метод наименьших квадратов с параметром $\lambda$ задается выражением

$j(\theta)=\sum_{t=1}^{n}(y_{t}-\theta^{T}\phi(\textbf{x}))^{2}+\lambda\left \| \theta \right \|^{2} (23)$

Данная запись получена по аналогии с  регуляризацией метода максимального правдоподобия (смотрите предыдущую лекцию). 

Эффект от регуляризатора состоит в том, что все параметры по возможности стремятся к нулю.

Так же линейные размерности пространства параметров, которые не относятся к обучающим признакам устанавливаются равными нулю.

Таким образом, мы ожидаем, что оптимальные параметры лежат внутри линейной оболочки, образованной векторами признаков, соответствующих обучающим примерам. Это действительно так.

Мы продолжим изучение ядровых функций (и их связь со скалярным произведением) в модели линейной регрессии в следующий раз.

Внимание! Этот перевод, возможно, ещё не готов.
Его статус: идёт перевод

Переведено на Нотабеноиде
http://translate.kursomir.ru/book/105/874

Переводчики: chargered

